RLHF reward modeling is implicitly doing social choiceâ€”in particular, something like Borda count. An alternative setup that more closely mimics the social choice formalism allows us to more considerately choose our preference aggregation rule. This alternative setup also allows us to handle the typical case of incomplete rater preference data better. Both preference aggregation rule generalization and incomplete data handling improvements plausibly have safety benefits.